---
layout: default
title: Neuro-Symbolic Video Question Answering
---

<style>
  p {
      font-size: 1.2em;
      line-height: 1.2em;
      color: #333;
  }

  * {
      margin: 0;
      padding: 0;
  }

  ol.q {list-style-type: upper-roman;}

  h1 {font-size: 40px;}
</style>

<body>
  <div class="blurb">
    <h1>Neuro-Symbolic Video Question Answering</h1>
    <br>
      <p align="justify">
        During my MSc in AI and ML at Imperial College London, I explored
        the task of video understanding through the proxy task of Video Question
        Answering (VideoQA). My objective was to investiagte ways to equip an
        AI model with visual reasoning abilities: perception, and causal and temporal reasoning. <br>
        <br>
        The VideoQA task I selected was that from the
        <a href="http://clevrer.csail.mit.edu/" target="_blank">CLEVRER</a>: Collision
        Events for Video Representation and Reasoning dataset;
        developed by the <a href="https://mitibmwatsonailab.mit.edu/research/" target="_blank">
        MIT-Watson Lab</a>. The dataset is composed of videos exhibiting
        motion and collision between objects on a constant background, whilst
        the questions require causal and temporal reasoning to be answered. <br>
        <br>
        I held this research under the supervision of <a href="https://wp.doc.ic.ac.uk/arusso/" target="_blank">
        Professor Alessandra Russo</a> and <a href="https://www.doc.ic.ac.uk/~nuric/pages/research.html" target="_blank">
        Nuri Cingillioglu</a>, with whom I will be sharing my work
        as a full paper and oral presentation in the <a href="https://aaai.org/Conferences/AAAI-21/ws21workshops/" target="_blank">
        Thirty-Fifth AAAI Conference</a> Workshop on <a href="https://sites.google.com/view/aaai2021workshop/home" target="_blank">
        Hybrid Artificial Intelligence</a>. Our the paper is available
        <a href="https://arxiv.org/abs/2101.06644" target="_blank">online</a>. <br><br>
      </p>
    <p>
      <img src="/gifs/svp_video_9000_description_6.gif" class="center">
    </p>
    <p class="blocktext" align="justify">
          Video of the <a href="http://clevrer.csail.mit.edu/" target="_blank">CLEVRER dataset</a>
          annotated by our model. The object descriptions and events are
          extracted from the modelâ€™s reasoning.
    </p>

    <h2> <u> Summary </u> </h2>
      <p align="justify">
        We developed a neuro-symbolic framework for the task of VideoQA, named the
        HySTER: Hybrid Spatio-Temporal Event Reasoner, which we
        tested on the CLEVRER datatset. <br>
        <br>
        Our model leverage the strength of deep learning through Mask R-CNN and
        ResNets in order to extract information from low-level video frames and
        ground a symbolic representation of the objects along with their position
        in the scene. The symbolic component, implemented under the form of Answer
        Set Programming (ASP) using clingo, then performs temporal and causal reasoning
        over the reconstructed scene, detecting the events occurring and answering
        the questions. <br>
        <br>
      </p>

    <h2> <u> Methodology </u> </h2>
    <p align="justify">
      An overview of the overall framework is presented in the following figure, and comprises
      three main components.
    </p>
    <br>

    <div style="font-size:1.2em">
        <ol style="list-style-type:upper-roman">
          <li>
            <b> Video Parser:</b> detects, classifies and localises the objects in the scene.
            This infromation is then translated into a symbolic representation.
          </li>
          <br>
          <li>
            <b> Question Parser: </b> translates the natural language questions into
            logic queries.
          </li>
          <br>
          <li>
            <b> Symbolic Reasoner: </b> detects events from the abstract
            scene representation built by the <b> Video Parser</b>, and reasons over them
            to answer the queries.
          </li>
        </ol>
    </div>
    <br>
    <p>
      <img src="/images/ns_vqa/architecture.png" class="center7">
    </p>
    <p class="blocktext" align="justify">
          Overview of the model architecture composed of 3 main components: <b>I</b>.
          Video Parser, <b>II</b>. Question Parser, <b>III</b>. Symbolic Reasoner. The yellow
          shaded parts refer to general components transferable between tasks.
          The green shaded parts refer to task specific symbolic components and
          information.
    </p>
    <br>
    <p align="justify">
    We will soon upload our code and examples of the ASP programs built to solve the task.
    </p>
    <br>
    <h2> <u> Results </u> </h2>
    <p>
      Find the results on the <a href="https://evalai.cloudcv.org/web/challenges/challenge-page/667/leaderboard/1813" target="_blank">
      CLEVRER challenge leaderboard </a>, under the label <em>TS_NS_IMPERIAL</em>,
      where we outperfromed the baselines set in the
      <a href="https://arxiv.org/abs/1910.01442" target="_blank">CLEVRER paper</a>.<br>
      <br>
      (Please note that the results in the leaderboard correspond to our best overall
      score, not the best performance in each question type, hence the difference
      with the values in our paper).
    </p>
    <br>
    <h2> <u> Next Steps </u> </h2>
    <p align="justify">
    We aim to explore the potential of using inductive logic programming and
    <a href="http://ilasp.com/?no_animation" target="_blank"><img src="../images/logos/ilasp.png"width=30></a>
    to reduce the amount of the human engineering in the process, allowing the model to learn event detection
    rules directly from abstract scene representations and question-answer pairs.
    Please <a href="mailto:theophile.sautory@gmail.com">contact me</a>
    for details concerning our symbolic rule learning methodology and initial results.
    </p>
  </div>
  <br>
</body>
